{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label conversion options\n",
    "convert_labels = True\n",
    "label_conversion_list = {\n",
    "    'subject':'SUB',\n",
    "    'subject.reference':'SUB',\n",
    "    \n",
    "    'coverage.temporal':'PER',\n",
    "    'coverage.temporal.reference':'PER',\n",
    "    'date.event':'PER',\n",
    "    'date.publication':'PER',\n",
    "    \n",
    "    'coverage.spatial.placename':'LOC',\n",
    "    'coverage.spatial.placename.reference':'LOC'\n",
    "}\n",
    "\n",
    "# output options\n",
    "token_label_separator = ' \\t' # doccano expects space + tab for sep!\n",
    "\n",
    "# define input/output folders\n",
    "ariadne_folder = \"D:\\\\phd-data\\\\NER-annotation-data\\\\English\\\\Ariadne-dataset-annotated\\\\Training&TaggedSample\\\\training-other\\\\\"\n",
    "output_folder = \"D:\\\\phd-data\\\\NER-annotation-data\\\\English\\\\Ariadne-dataset-annotated\\\\doccano-formatted\\\\training-other\\\\\"\n",
    "\n",
    "# define regex for matching 1 or more spaces\n",
    "regex_spaces = r\"[ ]+\"\n",
    "\n",
    "# define regex for matching 1 or more line endings\n",
    "regex_lineendings = r\"[\\n]+\"\n",
    "regex_lineendings_with_space = r\"[\\n]+ ?[\\n]+\"\n",
    "\n",
    "# define regex for matching tags\n",
    "regex_tag_start = r\"\\<([a-z.]+)\\>\"\n",
    "regex_tag_end = r\"\\<\\/([a-z.]+)\\>\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_tags = {}\n",
    "\n",
    "for filename in os.listdir(ariadne_folder):\n",
    "    \n",
    "    file_path = os.path.join(ariadne_folder, filename)\n",
    "    \n",
    "    with open(file_path) as f:\n",
    "        \n",
    "        text = f.read()\n",
    "        #print(text[0:200])\n",
    "        \n",
    "        # remove line endings\n",
    "        text = text.replace('\\n',' ')\n",
    "        \n",
    "        # replace multiple spaces with one space\n",
    "        text = re.sub(regex_spaces, \" \", text, 0, re.MULTILINE)\n",
    "        #print(text[0:200])\n",
    "        \n",
    "        # sentence detection / tokenisation\n",
    "        tokens_sentences = [word_tokenize(t) for t in sent_tokenize(text)]\n",
    "        \n",
    "        # re-assemble the tags\n",
    "        tokens_sentences_with_fixed_tags = []\n",
    "        \n",
    "        for sentence in tokens_sentences:\n",
    "            \n",
    "            tokens_with_fixed_tags = []\n",
    "            in_tag = False\n",
    "            fixed_tag = ''\n",
    "            \n",
    "            for token in sentence:\n",
    "                \n",
    "                # start of tag, start collecting tokens\n",
    "                if token == '<':\n",
    "                    in_tag = True\n",
    "                    fixed_tag = token\n",
    "                    \n",
    "                # end of tag, add collected tokens to output list\n",
    "                elif in_tag and token == '>':\n",
    "                    fixed_tag += token\n",
    "                    tokens_with_fixed_tags.append(fixed_tag)\n",
    "                    fixed_tag = ''\n",
    "                    in_tag = False\n",
    "                \n",
    "                # middle of tag, add to fixed_tag\n",
    "                elif in_tag:\n",
    "                    fixed_tag += token\n",
    "                \n",
    "                # normal token, add to output list\n",
    "                else:\n",
    "                    tokens_with_fixed_tags.append(token)\n",
    "            \n",
    "            # add fixed tags tokens to output\n",
    "            tokens_sentences_with_fixed_tags.append(tokens_with_fixed_tags)\n",
    "                    \n",
    "        \n",
    "        #print(tokens_sentences_with_fixed_tags[0:500])\n",
    "        #exit()\n",
    "        \n",
    "        \n",
    "        # loop through sentences/tokens, and add to output in BIO format\n",
    "        \n",
    "        output = ''\n",
    "        in_tag = False\n",
    "        first_token_in_tag = False\n",
    "        \n",
    "        for sentence in tokens_sentences_with_fixed_tags:\n",
    "            for token in sentence:\n",
    "                \n",
    "                # start tag, extract it\n",
    "                if re.match(regex_tag_start, token):\n",
    "                    tag = re.search(regex_tag_start, token).group(1)\n",
    "                    #all_tags[tag] = 1\n",
    "                    if tag in label_conversion_list:\n",
    "                        tag = label_conversion_list[tag]\n",
    "                        in_tag = True\n",
    "                        first_token_in_tag = True\n",
    "\n",
    "                # end tag, stop adding B/I\n",
    "                elif re.match(regex_tag_end, token):\n",
    "                    in_tag = False\n",
    "\n",
    "                # we're within a tag, add B/I\n",
    "                elif in_tag:\n",
    "                    if first_token_in_tag:\n",
    "                        output += f\"{token}{token_label_separator}B-{tag}\\n\"\n",
    "                        first_token_in_tag = False\n",
    "                    else:\n",
    "                        output += f\"{token}{token_label_separator}I-{tag}\\n\"\n",
    "\n",
    "                # token outside of tags, just add to output\n",
    "                else:\n",
    "                    output += f\"{token}{token_label_separator}O\\n\"\n",
    "\n",
    "            \n",
    "            # at the end of each 'sentence', if not within a tag, add a sentence separator. if in tag, merge two sentences\n",
    "            if not in_tag:\n",
    "                output += '\\n'\n",
    "        \n",
    "        \n",
    "        #print(output)\n",
    "        \n",
    "        # save BIO file\n",
    "        output_path = os.path.join(output_folder, filename)\n",
    "        with open(output_path, 'w') as f:\n",
    "            f.write(output)\n",
    "        \n",
    "\n",
    "#print(all_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
